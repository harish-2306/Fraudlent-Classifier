{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\nimport matplotlib.patches as mpatches\nimport time\n\n# Classifier Libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport collections\n\n\n# Other Libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\nfrom collections import Counter\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\ndf = pd.read_csv('../input/creditcard.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"376ce881-463a-4a09-9ac0-c63f85577eec","_uuid":"93031e732e5aca3a2b4984799d6bf58d76e4b52d","trusted":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"03ddb929-5bc8-4af4-90cd-21dcbb57560d","_uuid":"38bec67888aa534e9739e95ef9fac62d27a87021","trusted":true},"cell_type":"code","source":"# Good No Null Values!\ndf.isnull().sum().max()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"6a526b6c-8463-4f6f-92b0-e8a3a21cbb2e","_uuid":"479a5f12d3dd68262316a17b4b7b3499e0a2cbe0","trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"01c007fa-0fcc-4eea-84ff-0861a2f8c533","_uuid":"f6b96ff34855e3bf7af1f6979342b01c473e4e07","trusted":true},"cell_type":"code","source":"# The classes are heavily skewed we need to solve this issue later.\nprint('No Frauds', round(df['Class'].value_counts()[0]/len(df) * 100,2), '% of the dataset')\nprint('Frauds', round(df['Class'].value_counts()[1]/len(df) * 100,2), '% of the dataset')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"657bc987-4b15-4cfa-b290-c39a2632e2ac","_uuid":"337caaf6ed3f65beedb24a74eebb22d97ff52ba4","trusted":true},"cell_type":"code","source":"colors = [\"#0101DF\", \"#DF0101\"]\n\nsns.countplot('Class', data=df, palette=colors)\nplt.title('Class Distributions \\n (0: No Fraud || 1: Fraud)', fontsize=14)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"cee315f2-325f-42b6-a640-736f10c272cc","_uuid":"cfa51792bf6f8a6b318ae1bffcff4e922b1d1917","trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(18,4))\n\namount_val = df['Amount'].values\ntime_val = df['Time'].values\n\nsns.distplot(amount_val, ax=ax[0], color='r')\nax[0].set_title('Distribution of Transaction Amount', fontsize=14)\nax[0].set_xlim([min(amount_val), max(amount_val)])\n\nsns.distplot(time_val, ax=ax[1], color='b')\nax[1].set_title('Distribution of Transaction Time', fontsize=14)\nax[1].set_xlim([min(time_val), max(time_val)])\n\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"d5d64bf0-2fbb-4096-a265-f68887bf2fde","_uuid":"1501ec379c9b5c39c3857ba0febd0aedee9c30d5","trusted":true},"cell_type":"code","source":"# Since most of our data has already been scaled we should scale the columns that are left to scale (Amount and Time)\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\n\n# RobustScaler is less prone to outliers.\n\nstd_scaler = StandardScaler()\nrob_scaler = RobustScaler()\n\ndf['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\ndf['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n\ndf.drop(['Time','Amount'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"cdb9bb1e-9fab-4fd1-a409-468ba8bc36ee","_uuid":"a33d701247ab45d849c5e94735346a738a6c6970","trusted":true},"cell_type":"code","source":"scaled_amount = df['scaled_amount']\nscaled_time = df['scaled_time']\n\ndf.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\ndf.insert(0, 'scaled_amount', scaled_amount)\ndf.insert(1, 'scaled_time', scaled_time)\n\n# Amount and Time are Scaled!\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"c6c962cc-6f38-4a00-bcd7-ce9d91db954c","_uuid":"9f7b5d920703b3a3c8c0f62bc6042e4615bc8324","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nprint('No Frauds', round(df['Class'].value_counts()[0]/len(df) * 100,2), '% of the dataset')\nprint('Frauds', round(df['Class'].value_counts()[1]/len(df) * 100,2), '% of the dataset')\n\nX = df.drop('Class', axis=1)\ny = df['Class']\n\nsss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n\nfor train_index, test_index in sss.split(X, y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n\n# We already have X_train and y_train for undersample data thats why I am using original to distinguish and to not overwrite these variables.\n# original_Xtrain, original_Xtest, original_ytrain, original_ytest = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Check the Distribution of the labels\n\n\n# Turn into an array\noriginal_Xtrain = original_Xtrain.values\noriginal_Xtest = original_Xtest.values\noriginal_ytrain = original_ytrain.values\noriginal_ytest = original_ytest.values\n\n# See if both the train and test label distribution are similarly distributed\ntrain_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\ntest_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\nprint('-' * 100)\n\nprint('Label Distributions: \\n')\nprint(train_counts_label/ len(original_ytrain))\nprint(test_counts_label/ len(original_ytest))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"f0acfc44-eb2a-4356-ad03-d0c12807acd7","_uuid":"e3a2b89752681164f14c8273452fc66734d7f41b","trusted":true},"cell_type":"code","source":"# Since our classes are highly skewed we should make them equivalent in order to have a normal distribution of the classes.\n\n# Lets shuffle the data before creating the subsamples\n\ndf = df.sample(frac=1)\n\n# amount of fraud classes 492 rows.\nfraud_df = df.loc[df['Class'] == 1]\nnon_fraud_df = df.loc[df['Class'] == 0][:492]\n\nnormal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n\n# Shuffle dataframe rows\nnew_df = normal_distributed_df.sample(frac=1, random_state=42)\n\nnew_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"73454100-dc69-49fd-b1b2-f72e326bca5d","_uuid":"68b42e92df59f10fbd3ba700389796c4506af604","trusted":true},"cell_type":"code","source":"print('Distribution of the Classes in the subsample dataset')\nprint(new_df['Class'].value_counts()/len(new_df))\n\n\n\nsns.countplot('Class', data=new_df, palette=colors)\nplt.title('Equally Distributed Classes', fontsize=14)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"f83cde6b-90d0-4e9d-ac63-fb69780431b2","_uuid":"af3027e7df67b75c92c88d597003632e285c9bff","trusted":true},"cell_type":"code","source":"# New_df is from the random undersample data (fewer instances)\nX = new_df.drop('Class', axis=1)\ny = new_df['Class']\n\n\n# T-SNE Implementation\nt0 = time.time()\nX_reduced_tsne = TSNE(n_components=2, random_state=42).fit_transform(X.values)\nt1 = time.time()\nprint(\"T-SNE took {:.2} s\".format(t1 - t0))\n\n# PCA Implementation\nt0 = time.time()\nX_reduced_pca = PCA(n_components=2, random_state=42).fit_transform(X.values)\nt1 = time.time()\nprint(\"PCA took {:.2} s\".format(t1 - t0))\n\n# TruncatedSVD\nt0 = time.time()\nX_reduced_svd = TruncatedSVD(n_components=2, algorithm='randomized', random_state=42).fit_transform(X.values)\nt1 = time.time()\nprint(\"Truncated SVD took {:.2} s\".format(t1 - t0))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"07015ae5-f7ac-4d64-8f41-1e4b7c9dd2ac","_uuid":"084f2a7421c2212082491d2a90e65d65c52b434a","trusted":true},"cell_type":"code","source":"f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24,6))\n# labels = ['No Fraud', 'Fraud']\nf.suptitle('Clusters using Dimensionality Reduction', fontsize=14)\n\n\nblue_patch = mpatches.Patch(color='#0A0AFF', label='No Fraud')\nred_patch = mpatches.Patch(color='#AF0000', label='Fraud')\n\n\n# t-SNE scatter plot\nax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nax1.set_title('t-SNE', fontsize=14)\n\nax1.grid(True)\n\nax1.legend(handles=[blue_patch, red_patch])\n\n\n# PCA scatter plot\nax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nax2.set_title('PCA', fontsize=14)\n\nax2.grid(True)\n\nax2.legend(handles=[blue_patch, red_patch])\n\n# TruncatedSVD scatter plot\nax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nax3.set_title('Truncated SVD', fontsize=14)\n\nax3.grid(True)\n\nax3.legend(handles=[blue_patch, red_patch])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"85ce8738-7599-4b06-a722-5c0ed073599b","_uuid":"e3751d88766a982119e522e27a9c0c647f20af85","trusted":true},"cell_type":"code","source":"# Undersampling before cross validating (prone to overfit)\nX = new_df.drop('Class', axis=1)\ny = new_df['Class']","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"288a65b7-8b86-44b1-973d-38dbcfe82bbb","_uuid":"fb0a479efaa7147d6702c2c24083f1118621863f","trusted":true},"cell_type":"code","source":"# Our data is already scaled we should split our training and test sets\nfrom sklearn.model_selection import train_test_split\n\n# This is explicitly used for undersampling.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"bccd5685-a979-451e-85b3-1cb968523540","_uuid":"28f5178089d2d133b9e7478c1c7dc7a1f98aabee","trusted":true},"cell_type":"code","source":"# Turn the values into an array for feeding the classification algorithms.\nX_train = X_train.values\nX_test = X_test.values\ny_train = y_train.values\ny_test = y_test.values","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"7810d0b9-b4e5-4b7f-909b-c127365b167c","_uuid":"8dd4ea07fd60973fccabc2d46af28a09b0de9178","trusted":true},"cell_type":"code","source":"# Let's implement simple classifiers\n\nclassifiers = {\n    \"LogisiticRegression\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier(),\n    \"Support Vector Classifier\": SVC(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n}","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"eb37c0f6-9cfe-48b6-92d3-475d5e6767a6","_uuid":"fe129af379caccc5428cf1836e6c96bd32e68feb","trusted":true},"cell_type":"code","source":"# Wow our scores are getting even high scores even when applying cross validation.\nfrom sklearn.model_selection import cross_val_score\n\n\nfor key, classifier in classifiers.items():\n    classifier.fit(X_train, y_train)\n    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"a1c35773-f4c7-4caf-9911-532784c9eae0","_uuid":"d15b1ab16737358806e34c48dc57aa238cf0cfd2","trusted":true},"cell_type":"code","source":"# Use GridSearchCV to find the best parameters.\nfrom sklearn.model_selection import GridSearchCV\n\n\n# Logistic Regression \nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\n\n\ngrid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\ngrid_log_reg.fit(X_train, y_train)\n# We automatically get the logistic regression with the best parameters.\nlog_reg = grid_log_reg.best_estimator_\n\nknears_params = {\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n\ngrid_knears = GridSearchCV(KNeighborsClassifier(), knears_params)\ngrid_knears.fit(X_train, y_train)\n# KNears best estimator\nknears_neighbors = grid_knears.best_estimator_\n\n# Support Vector Classifier\nsvc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\ngrid_svc = GridSearchCV(SVC(), svc_params)\ngrid_svc.fit(X_train, y_train)\n\n# SVC best estimator\nsvc = grid_svc.best_estimator_\n\n# DecisionTree Classifier\ntree_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)), \n              \"min_samples_leaf\": list(range(5,7,1))}\ngrid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params)\ngrid_tree.fit(X_train, y_train)\n\n# tree best estimator\ntree_clf = grid_tree.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"7f327bcd-335f-4e49-af07-fc4214dbcbdc","_uuid":"1b2108bf377b924ed8a6efe580d9e162a132cd9e","trusted":true},"cell_type":"code","source":"# Overfitting Case\n\nlog_reg_score = cross_val_score(log_reg, X_train, y_train, cv=5)\nprint('Logistic Regression Cross Validation Score: ', round(log_reg_score.mean() * 100, 2).astype(str) + '%')\n\n\nknears_score = cross_val_score(knears_neighbors, X_train, y_train, cv=5)\nprint('Knears Neighbors Cross Validation Score', round(knears_score.mean() * 100, 2).astype(str) + '%')\n\nsvc_score = cross_val_score(svc, X_train, y_train, cv=5)\nprint('Support Vector Classifier Cross Validation Score', round(svc_score.mean() * 100, 2).astype(str) + '%')\n\ntree_score = cross_val_score(tree_clf, X_train, y_train, cv=5)\nprint('DecisionTree Classifier Cross Validation Score', round(tree_score.mean() * 100, 2).astype(str) + '%')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"38e430ef-0160-47a1-9b6f-11ff62c5ecc0","_uuid":"eeb5736b279bb8fa3804689a175394f216ec4f72","trusted":true},"cell_type":"code","source":"# We will undersample during cross validating\nundersample_X = df.drop('Class', axis=1)\nundersample_y = df['Class']\n\nfor train_index, test_index in sss.split(undersample_X, undersample_y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    undersample_Xtrain, undersample_Xtest = undersample_X.iloc[train_index], undersample_X.iloc[test_index]\n    undersample_ytrain, undersample_ytest = undersample_y.iloc[train_index], undersample_y.iloc[test_index]\n    \nundersample_Xtrain = undersample_Xtrain.values\nundersample_Xtest = undersample_Xtest.values\nundersample_ytrain = undersample_ytrain.values\nundersample_ytest = undersample_ytest.values \n\nundersample_accuracy = []\nundersample_precision = []\nundersample_recall = []\nundersample_f1 = []\nundersample_auc = []\n\n# Implementing NearMiss Technique \n# Distribution of NearMiss (Just to see how it distributes the labels we won't use these variables)\nX_nearmiss, y_nearmiss = NearMiss().fit_sample(undersample_X.values, undersample_y.values)\nprint('NearMiss Label Distribution: {}'.format(Counter(y_nearmiss)))\n# Cross Validating the right way\n\nfor train, test in sss.split(undersample_Xtrain, undersample_ytrain):\n    undersample_pipeline = imbalanced_make_pipeline(NearMiss(sampling_strategy='majority'), log_reg) # SMOTE happens during Cross Validation not before..\n    undersample_model = undersample_pipeline.fit(undersample_Xtrain[train], undersample_ytrain[train])\n    undersample_prediction = undersample_model.predict(undersample_Xtrain[test])\n    \n    undersample_accuracy.append(undersample_pipeline.score(original_Xtrain[test], original_ytrain[test]))\n    undersample_precision.append(precision_score(original_ytrain[test], undersample_prediction))\n    undersample_recall.append(recall_score(original_ytrain[test], undersample_prediction))\n    undersample_f1.append(f1_score(original_ytrain[test], undersample_prediction))\n    undersample_auc.append(roc_auc_score(original_ytrain[test], undersample_prediction))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"bb72803c-3ea3-40cd-8ac3-399540ab7f5a","_uuid":"a12fb2f7e104931bb78e1bd6cfc5a516c970708b","trusted":true},"cell_type":"code","source":"# Let's Plot LogisticRegression Learning Curve\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import learning_curve\n\ndef plot_learning_curve(estimator1, estimator2, estimator3, estimator4, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(20,14), sharey=True)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    # First Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator1, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax1.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax1.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax1.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax1.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax1.set_title(\"Logistic Regression Learning Curve\", fontsize=14)\n    ax1.set_xlabel('Training size (m)')\n    ax1.set_ylabel('Score')\n    ax1.grid(True)\n    ax1.legend(loc=\"best\")\n    \n    # Second Estimator \n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator2, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax2.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax2.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax2.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax2.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax2.set_title(\"Knears Neighbors Learning Curve\", fontsize=14)\n    ax2.set_xlabel('Training size (m)')\n    ax2.set_ylabel('Score')\n    ax2.grid(True)\n    ax2.legend(loc=\"best\")\n    \n    # Third Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator3, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax3.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax3.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax3.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax3.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax3.set_title(\"Support Vector Classifier \\n Learning Curve\", fontsize=14)\n    ax3.set_xlabel('Training size (m)')\n    ax3.set_ylabel('Score')\n    ax3.grid(True)\n    ax3.legend(loc=\"best\")\n    \n    # Fourth Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator4, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax4.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax4.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax4.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax4.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax4.set_title(\"Decision Tree Classifier \\n Learning Curve\", fontsize=14)\n    ax4.set_xlabel('Training size (m)')\n    ax4.set_ylabel('Score')\n    ax4.grid(True)\n    ax4.legend(loc=\"best\")\n    return plt","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"5b8302aa-0207-455f-8c1a-78ff3e9b5141","_uuid":"15b262baa0c61c288a5453031b4d7f80f5a7a5ab","trusted":true},"cell_type":"code","source":"cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\nplot_learning_curve(log_reg, knears_neighbors, svc, tree_clf, X_train, y_train, (0.87, 1.01), cv=cv, n_jobs=4)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"780e485a-ea64-48a0-ad97-a7516b047f32","_uuid":"fdd59bf2c7a8e61cfb401142570643e8a29cf86b","trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve\nfrom sklearn.model_selection import cross_val_predict\n# Create a DataFrame with all the scores and the classifiers names.\n\nlog_reg_pred = cross_val_predict(log_reg, X_train, y_train, cv=5,\n                             method=\"decision_function\")\n\nknears_pred = cross_val_predict(knears_neighbors, X_train, y_train, cv=5)\n\nsvc_pred = cross_val_predict(svc, X_train, y_train, cv=5,\n                             method=\"decision_function\")\n\ntree_pred = cross_val_predict(tree_clf, X_train, y_train, cv=5)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"57c211c6-e88f-4634-b321-4949df08815d","_uuid":"cb2e4715e91e36f2029ef2a5c241991ff162cd9f","trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\n\nprint('Logistic Regression: ', roc_auc_score(y_train, log_reg_pred))\nprint('KNears Neighbors: ', roc_auc_score(y_train, knears_pred))\nprint('Support Vector Classifier: ', roc_auc_score(y_train, svc_pred))\nprint('Decision Tree Classifier: ', roc_auc_score(y_train, tree_pred))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"89b0b9b6-ef82-4b69-9517-e89a79696dbb","_uuid":"9d57aad23f3f72f3c45bf80b089a65acbce2a9ab","trusted":true},"cell_type":"code","source":"log_fpr, log_tpr, log_thresold = roc_curve(y_train, log_reg_pred)\nknear_fpr, knear_tpr, knear_threshold = roc_curve(y_train, knears_pred)\nsvc_fpr, svc_tpr, svc_threshold = roc_curve(y_train, svc_pred)\ntree_fpr, tree_tpr, tree_threshold = roc_curve(y_train, tree_pred)\n\n\ndef graph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr):\n    plt.figure(figsize=(16,8))\n    plt.title('ROC Curve \\n Top 4 Classifiers', fontsize=18)\n    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y_train, log_reg_pred)))\n    plt.plot(knear_fpr, knear_tpr, label='KNears Neighbors Classifier Score: {:.4f}'.format(roc_auc_score(y_train, knears_pred)))\n    plt.plot(svc_fpr, svc_tpr, label='Support Vector Classifier Score: {:.4f}'.format(roc_auc_score(y_train, svc_pred)))\n    plt.plot(tree_fpr, tree_tpr, label='Decision Tree Classifier Score: {:.4f}'.format(roc_auc_score(y_train, tree_pred)))\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.01, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n                )\n    plt.legend()\n    \ngraph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"cc175ddc-ddd7-4087-ae1f-dd6fac664d58","_uuid":"96f8d3f4160d65f12af4c7106739c4ad46d1e76b","trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\n\n\nprint('Length of X (train): {} | Length of y (train): {}'.format(len(original_Xtrain), len(original_ytrain)))\nprint('Length of X (test): {} | Length of y (test): {}'.format(len(original_Xtest), len(original_ytest)))\n\n# List to append the score and then find the average\naccuracy_lst = []\nprecision_lst = []\nrecall_lst = []\nf1_lst = []\nauc_lst = []\n\n# Classifier with optimal parameters\n# log_reg_sm = grid_log_reg.best_estimator_\nlog_reg_sm = LogisticRegression()\n\n\n\n\nrand_log_reg = RandomizedSearchCV(LogisticRegression(), log_reg_params, n_iter=4)\n\n\n# Implementing SMOTE Technique \n# Cross Validating the right way\n# Parameters\nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\nfor train, test in sss.split(original_Xtrain, original_ytrain):\n    pipeline = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_log_reg) # SMOTE happens during Cross Validation not before..\n    model = pipeline.fit(original_Xtrain[train], original_ytrain[train])\n    best_est = rand_log_reg.best_estimator_\n    prediction = best_est.predict(original_Xtrain[test])\n    \n    accuracy_lst.append(pipeline.score(original_Xtrain[test], original_ytrain[test]))\n    precision_lst.append(precision_score(original_ytrain[test], prediction))\n    recall_lst.append(recall_score(original_ytrain[test], prediction))\n    f1_lst.append(f1_score(original_ytrain[test], prediction))\n    auc_lst.append(roc_auc_score(original_ytrain[test], prediction))\n    \nprint('---' * 45)\nprint('')\nprint(\"accuracy: {}\".format(np.mean(accuracy_lst)))\nprint(\"precision: {}\".format(np.mean(precision_lst)))\nprint(\"recall: {}\".format(np.mean(recall_lst)))\nprint(\"f1: {}\".format(np.mean(f1_lst)))\nprint('---' * 45)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"41dd6215-2927-4de3-999a-724272aea2b6","_uuid":"d109652d1e170d0f9938d64f29aa33d93c941cdc","trusted":true},"cell_type":"code","source":"labels = ['No Fraud', 'Fraud']\nsmote_prediction = best_est.predict(original_Xtest)\nprint(classification_report(original_ytest, smote_prediction, target_names=labels))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"f0e671f7-7ed1-4188-b9bf-e509f050b134","_uuid":"a8dcc4bba95aed7fbc8b9e39ceeeec6902d1865c","trusted":true},"cell_type":"code","source":"y_score = best_est.decision_function(original_Xtest)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"77bed8fa-1117-4bc0-a740-bd1bd97012a4","_uuid":"f9213b24dd2fb3eb04f9b59c3b715dcb167664b5","trusted":true},"cell_type":"code","source":"average_precision = average_precision_score(original_ytest, y_score)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"54e926f4-2a5d-4bb1-b74c-8cd79da7b6e5","_uuid":"7be0445ac80df7ca252ec350b026d6275669aea6","trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12,6))\n\nprecision, recall, _ = precision_recall_curve(original_ytest, y_score)\n\nplt.step(recall, precision, color='r', alpha=0.2,\n         where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.2,\n                 color='#F59B00')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('OverSampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n          average_precision), fontsize=16)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"d5c6fe5b-f086-4151-aba5-3c758677be0f","_uuid":"787ec6bb25c3dc379c12a57619f5cc3e41afa42e","trusted":true},"cell_type":"code","source":"# SMOTE Technique (OverSampling) After splitting and Cross Validating\nsm = SMOTE(ratio='minority', random_state=42)\n# Xsm_train, ysm_train = sm.fit_sample(X_train, y_train)\n\n\n# This will be the data were we are going to \nXsm_train, ysm_train = sm.fit_sample(original_Xtrain, original_ytrain)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"7af62152-e7e3-45c8-9a56-69467ede59a6","_uuid":"a25f7cc327bbaeae985cb0d2f9a0c8e2c2009aa3","trusted":true},"cell_type":"code","source":"# We Improve the score by 2% points approximately \n# Implement GridSearchCV and the other models.\n\n# Logistic Regression\nt0 = time.time()\nlog_reg_sm = grid_log_reg.best_estimator_\nlog_reg_sm.fit(Xsm_train, ysm_train)\nt1 = time.time()\nprint(\"Fitting oversample data took :{} sec\".format(t1 - t0))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"e774e22e-8ce0-4c2e-99fa-9f3c6a915b6d","_uuid":"35be99c61da4054c952e1955a5e809d003966975","trusted":true},"cell_type":"code","source":"import keras\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Activation\nfrom keras.layers.core import Dense\nfrom keras.optimizers import Adam\nfrom keras.metrics import categorical_crossentropy\n\nn_inputs = X_train.shape[1]\n\nundersample_model = Sequential([\n    Dense(n_inputs, input_shape=(n_inputs, ), activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(2, activation='softmax')\n])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"c249283c-f4d9-43f4-a859-ccd5a7661cf7","_uuid":"ccdae6b84326551e1ff5199c44f7d53ccd3179d9","trusted":true},"cell_type":"code","source":"undersample_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"6327357a-b8ca-4aa4-8764-48673b2d6c9d","_uuid":"e2ec864b9ef6f530df28688a703bcc8f2243baa1","trusted":true},"cell_type":"code","source":"undersample_model.compile(Adam(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"0067f625-d734-4c15-9526-6efb1c47dc2c","_uuid":"98a36722723d4f7285eb9de158b12be9694a603f","trusted":true},"cell_type":"code","source":"undersample_model.fit(X_train, y_train, validation_split=0.2, batch_size=25, epochs=20, shuffle=True, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"9a30cd49-23c3-4417-8ce1-b6e82890481c","_uuid":"e82f40ef343b53b71d6fcfa317e872278db27114","trusted":true},"cell_type":"code","source":"undersample_predictions = undersample_model.predict(original_Xtest, batch_size=200, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"a47a91a0-2cf3-436f-a1cd-bb207ba94997","_uuid":"028270a5b2ba0e4c85100f287fec64343ad900ea","trusted":true},"cell_type":"code","source":"undersample_fraud_predictions = undersample_model.predict_classes(original_Xtest, batch_size=200, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"44216511-5fa7-404f-9b40-2c72f30c1ca7","_uuid":"b0681d10d7f3e68a6b91864670b7aa04cacd362f","trusted":true},"cell_type":"code","source":"import itertools\n\n# Create a confusion matrix\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize=14)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"ee16d514-2134-4cfd-b4d8-1ddb183960f0","_uuid":"003c84c96d49bebdb5f09970102d89e3db5ff2f1","trusted":true},"cell_type":"code","source":"undersample_cm = confusion_matrix(original_ytest, undersample_fraud_predictions)\nactual_cm = confusion_matrix(original_ytest, original_ytest)\nlabels = ['No Fraud', 'Fraud']\n\nfig = plt.figure(figsize=(16,8))\n\nfig.add_subplot(221)\nplot_confusion_matrix(undersample_cm, labels, title=\"Random UnderSample \\n Confusion Matrix\", cmap=plt.cm.Reds)\n\nfig.add_subplot(222)\nplot_confusion_matrix(actual_cm, labels, title=\"Confusion Matrix \\n (with 100% accuracy)\", cmap=plt.cm.Greens)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"e7c29164-751a-4ccd-b517-527debf38fdf","_uuid":"7130856ed8a6f87fe86b72c5142ff27ccf4eef1a","trusted":true},"cell_type":"code","source":"n_inputs = Xsm_train.shape[1]\n\noversample_model = Sequential([\n    Dense(n_inputs, input_shape=(n_inputs, ), activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(2, activation='softmax')\n])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"25beccc6-fe0b-4c85-a439-81aaf5cdc019","_uuid":"937a0a57f3dae8172fb6c88b60944257e8198ae7","trusted":true},"cell_type":"code","source":"oversample_model.compile(Adam(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"888c3acd-3d8d-4f1b-b68a-bef304feca14","_uuid":"5ddbf33763fb33393d7969fdbd7338aa8e708c43","trusted":true},"cell_type":"code","source":"oversample_model.fit(Xsm_train, ysm_train, validation_split=0.2, batch_size=300, epochs=20, shuffle=True, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"09628790-3d4b-4516-b355-4177e5ad9329","_uuid":"222a3f8e7614c4e0241e0f0c85b0ecd45fd3cca6","trusted":true},"cell_type":"code","source":"oversample_predictions = oversample_model.predict(original_Xtest, batch_size=200, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"374217c9-a4b7-4691-918b-6a91bb1c02e3","_uuid":"9ede2c436251f560615087202cff031936f3a6e5","trusted":true},"cell_type":"code","source":"oversample_fraud_predictions = oversample_model.predict_classes(original_Xtest, batch_size=200, verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_cell_guid":"9a58d39f-9149-4279-bce3-fc8372e55f93","_uuid":"a18452b7051e4905f32b27940f006d0bc4bc2d5e","trusted":true},"cell_type":"code","source":"oversample_smote = confusion_matrix(original_ytest, oversample_fraud_predictions)\nactual_cm = confusion_matrix(original_ytest, original_ytest)\nlabels = ['No Fraud', 'Fraud']\n\nfig = plt.figure(figsize=(16,8))\n\nfig.add_subplot(221)\nplot_confusion_matrix(oversample_smote, labels, title=\"OverSample (SMOTE) \\n Confusion Matrix\", cmap=plt.cm.Oranges)\n\nfig.add_subplot(222)\nplot_confusion_matrix(actual_cm, labels, title=\"Confusion Matrix \\n (with 100% accuracy)\", cmap=plt.cm.Greens)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}